{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b388744-2586-4d03-a125-d7f7002031c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, countDistinct, broadcast, col\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "719c9059-5243-4518-a47e-b81f08ccf476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "zipcodes_df = spark.read.csv(\"/Volumes/mp_spark_deep_ddve/default/zipcode_data/zipcode_data.csv\", header=True, inferSchema=True)\n",
    "customers_df = spark.read.table(\"hive_metastore.default.fake_customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123e328b-7610-4df6-a7f7-e530407c14d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# zipcodes_df.show(100)\n",
    "\n",
    "agg_zipcodes_df = (zipcodes_df\n",
    "                    .withColumn(\"zipCode\", col(\"zipCode\").cast(\"string\"))\n",
    "                    .groupBy(\"zipCode\")\n",
    "                    .agg(sum(\"population\").alias(\"total_population\"))\n",
    ")\n",
    "\n",
    "# display(agg_zipcodes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1e8f151-2e18-49bb-a774-a0ef3dacf529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(5) HashAggregate(keys=[zip_code#1529], functions=[finalmerge_count(distinct merge count#2636L) AS count(customer_id#1522L)#2632L])\n+- Exchange hashpartitioning(zip_code#1529, 200), ENSURE_REQUIREMENTS, [plan_id=2539]\n   +- *(4) HashAggregate(keys=[zip_code#1529], functions=[partial_count(distinct customer_id#1522L) AS count#2636L])\n      +- *(4) HashAggregate(keys=[zip_code#1529, customer_id#1522L], functions=[])\n         +- Exchange hashpartitioning(zip_code#1529, customer_id#1522L, 200), ENSURE_REQUIREMENTS, [plan_id=2534]\n            +- *(3) HashAggregate(keys=[zip_code#1529, customer_id#1522L], functions=[])\n               +- *(3) Project [customer_id#1522L, zip_code#1529]\n                  +- *(3) BroadcastHashJoin [zip_code#1529], [zipCode#1540], Inner, BuildRight, false, false\n                     :- *(3) Project [customer_id#1522L, zip_code#1529]\n                     :  +- *(3) Filter (if (isnotnull(_databricks_internal_edge_computed_column_skip_row#2648)) (_databricks_internal_edge_computed_column_skip_row#2648 = false) else isnotnull(raise_error(DELTA_SKIP_ROW_COLUMN_NOT_FILLED, map(keys: [], values: []), NullType)) AND isnotnull(zip_code#1529))\n                     :     +- *(3) ColumnarToRow\n                     :        +- FileScan parquet hive_metastore.default.fake_customers[customer_id#1522L,zip_code#1529,_databricks_internal_edge_computed_column_skip_row#2648] Batched: true, DataFilters: [isnotnull(zip_code#1529)], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[dbfs:/user/hive/warehouse/fake_customers], PartitionFilters: [], PushedFilters: [IsNotNull(zip_code)], ReadSchema: struct<customer_id:bigint,zip_code:string,_databricks_internal_edge_computed_column_skip_row:bool...\n                     +- BroadcastExchange HashedRelationBroadcastMode(ArrayBuffer(input[0, string, true]),false), [plan_id=2528]\n                        +- *(2) HashAggregate(keys=[zipCode#1540], functions=[])\n                           +- Exchange hashpartitioning(zipCode#1540, 200), ENSURE_REQUIREMENTS, [plan_id=2524]\n                              +- *(1) HashAggregate(keys=[zipCode#1540], functions=[])\n                                 +- *(1) Project [cast(zipCode#1498 as string) AS zipCode#1540]\n                                    +- *(1) Filter (isnotnull(zipCode#1498) AND isnotnull(cast(zipCode#1498 as string)))\n                                       +- FileScan csv [zipCode#1498] Batched: false, DataFilters: [isnotnull(zipCode#1498), isnotnull(cast(zipCode#1498 as string))], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/Volumes/mp_spark_deep_ddve/default/zipcode_data/zipcode_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(zipCode)], ReadSchema: struct<zipCode:int>\n\n\n== Optimizer Statistics (table names per statistics state) ==\n  missing = fake_customers\n  partial = \n  full    = \nCorrective actions: consider running the following command on all tables with missing or partial statistics\n  ANALYZE TABLE <table-name> COMPUTE STATISTICS FOR ALL COLUMNS\n\n"
     ]
    }
   ],
   "source": [
    "joined_df = (customers_df\n",
    "            .join(broadcast(agg_zipcodes_df), customers_df.zip_code == agg_zipcodes_df.zipCode, how=\"inner\")\n",
    "            .groupby(\"zip_code\")\n",
    "            .agg(countDistinct(\"customer_id\").alias(\"total_customers\"))\n",
    ")\n",
    "        \n",
    "joined_df.explain()\n",
    "\n",
    "joined_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"hive_metastore.default.fake_broadcast_join_example\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "assignment_2_join_strategies",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}